{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment- TLinternship.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PYCyHM1tAyo"
      },
      "source": [
        "from warcio.archiveiterator import ArchiveIterator\n",
        "import gzip\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import Comment as cmnt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import urllib\n",
        "import urllib.request\n",
        "import os\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTHNmRiUtAyy"
      },
      "source": [
        "def get_url_list(target_url): \n",
        "    d={}\n",
        "    proxy=\"http://commoncrawl.s3.amazonaws.com/\"\n",
        "    stream = requests.get(proxy+target_url, stream=True).raw\n",
        "    i=0\n",
        "    for record in ArchiveIterator(stream):\n",
        "        if record.rec_type == 'response':\n",
        "                if check_if_covid(record.rec_headers.get_header('WARC-Target-URI'))==True:\n",
        "                  print(\"adding\")\n",
        "                  d[i]=(record.rec_headers.get_header('WARC-Target-URI'))\n",
        "                  i=i+1\n",
        "    return d"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQOfWRKAtAy2"
      },
      "source": [
        "\n",
        "def proper_text(inp):\n",
        "    if inp.parent.name in ['title', 'meta', '[document]','style', 'script', 'head']:\n",
        "        return False\n",
        "    if isinstance(inp, cmnt):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def text_from_html(body):\n",
        "    parsed = BeautifulSoup(body, 'html.parser')\n",
        "    text = parsed.findAll(text=True)\n",
        "    beauty_text = filter(proper_text, text)  \n",
        "    return u\" \".join(word.strip() for word in beauty_text)\n",
        "\n",
        "def get_content(url):\n",
        "    try:\n",
        "        user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "        headers={'User-Agent':user_agent,} \n",
        "        request=urllib.request.Request(url,None,headers) #The assembled request\n",
        "        response = urllib.request.urlopen(request)\n",
        "        data = response.read() #\n",
        "        s=text_from_html(data)\n",
        "    except:\n",
        "        s=\"error\"\n",
        "    return s\n",
        "        "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnlw2lDrtAy2"
      },
      "source": [
        "def check_if_covid(url):\n",
        "  s=get_content(url)\n",
        "  s1=word_tokenize(s)\n",
        "  s2=[]\n",
        "  for i in s1:\n",
        "      word=re.sub('[^A-Za-z0-9]+', '', i)\n",
        "      if len(word)>1:\n",
        "        if word in ['CORONA','coronavirus','corona','covid','covid-19','covid','Covid','Corona']:\n",
        "          s2.append(word)  \n",
        "  if len(s2)>1:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z10Fezdb6OuA"
      },
      "source": [
        "prefixed = [filename for filename in os.listdir('.') if filename.startswith(\"2020\")]\r\n",
        "\r\n",
        "url={}\r\n",
        "url[0]=0\r\n",
        "urls={}\r\n",
        "for i in range(len(prefixed)):\r\n",
        "  with gzip.open(prefixed[i], 'rb') as f:\r\n",
        "      for line in f.readlines():\r\n",
        "                if url[0]<=1000:\r\n",
        "                  op=get_url_list(line.decode()[0:-1])\r\n",
        "                  url[0]=url[0]+len(op)\r\n",
        "                urls[str(prefixed[i])]=list(op.values())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kt2A6KFQYWl"
      },
      "source": [
        "print(urls)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}